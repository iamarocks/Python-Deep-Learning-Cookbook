{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Recurrent Neural Networks, data is processed the same way for every element in a sequence, and the output depends on the previous computations. This structure has proven to be very powerful for many applications, such as for natural language processing (NLP) and time series predictions.\n",
    "![alt text][logo]\n",
    "\n",
    "[logo]: https://github.com/sara-kassani/Python-Deep-Learning-Cookbook/blob/master/data/RNN.png?raw=true \"RNNs\"\n",
    "As we can see in the figure, the output of a RNN does not onlydepend on the current input X t , but also on past inputs (X t-1 ). <br><br>\n",
    "Basically, this gives the network a type of memory. ; There are multiple types of RNNs where the input and output\n",
    "dimension can differ. An RNN can have one input variable and multiple output variables (1-to-n, with n > 1), multiple input variables and one output variable (n-to-1, with n > 1), or multiple input and output variables (n-to-m, with n,m > 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a simple RNN\n",
    "We start with implementing a simple form of a recurrent neural network to understand the basic idea of RNNs. In this example, we will feed the RNN four binary variables. These represent the weather types on a certain day. For example, [1, 0, 0, 0] stands for sunny and [1, 0, 1, 0] stands for sunny and windy. The target value is a double representing the percentage of rain on that day. For this problem, we can say that the quantity of rain on a certain day also depends on the values of the previous day. This makes this problem well suited for a 4-to-1 RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "X.append([1,0,0,0])\n",
    "X.append([0,1,0,0])\n",
    "X.append([0,0,1,0])\n",
    "X.append([0,0,0,1])\n",
    "X.append([0,0,0,1])\n",
    "X.append([1,0,0,0])\n",
    "X.append([0,1,0,0])\n",
    "X.append([0,0,1,0])\n",
    "X.append([0,0,0,1])\n",
    "\n",
    "y = [0.20, 0.30, 0.40, 0.50, 0.05, 0.10, 0.20, 0.30, 0.40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this regression problem, we will be using a sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return 1.0 - x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we initialize the layers and weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = []\n",
    "# 4 input variables, 10 hidden units and 1 output variable\n",
    "n_units = (4, 16, 1)\n",
    "n_layers = len(n_units)\n",
    "\n",
    "layers.append(np.ones(n_units[0]+1+n_units[1]))\n",
    "for i in range(1, n_layers):\n",
    "    layers.append(np.ones(n_units[i]))\n",
    "\n",
    "weights = []\n",
    "for i in range(n_layers-1):\n",
    "    weights.append(np.zeros((layers[i].size, layers[i+1].size)))\n",
    "\n",
    "weights_delta = [0,]*len(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are now ready to define the function for the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwards(data):\n",
    "    layers[0][:n_units[0]] = data\n",
    "    layers[0][n_units[0]:-1] = layers[1]\n",
    "\n",
    "    # Propagate the data forwards\n",
    "    for i in range(1, n_layers):\n",
    "        layers[i][...] = sigmoid(np.dot(layers[i-1], weights[i-1]))\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the backwards pass, we will determine the errors and update the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backwards(target, learning_rate=0.1, momentum=0.1):\n",
    "    deltas = []\n",
    "    error = target - layers[-1]\n",
    "    delta = error * sigmoid_der(layers[-1])\n",
    "    deltas.append(delta)\n",
    "\n",
    "    # Determine error in hidden layers\n",
    "    for i in range(n_layers-2, 0, -1):\n",
    "        delta = np.dot(deltas[0], weights[i].T) * sigmoid_der(layers[i])\n",
    "        deltas.insert(0, delta)\n",
    "\n",
    "    # Update weights\n",
    "    for i in range(len(weights)):\n",
    "        layer = np.atleast_2d(layers[i])\n",
    "        delta = np.atleast_2d(deltas[i])\n",
    "        weights_delta_temp = np.dot(layer.T, delta)\n",
    "        weights[i] += learning_rate*weights_delta_temp + momentum*weights_delta[i]\n",
    "        weights_delta[i] = weights_delta_temp\n",
    "\n",
    "    return (error**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our simple RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - loss: 0.3116\n",
      "epoch 1000 - loss: 0.1660\n",
      "epoch 2000 - loss: 0.1801\n",
      "epoch 3000 - loss: 0.1877\n",
      "epoch 4000 - loss: 0.1914\n",
      "epoch 5000 - loss: 0.1922\n",
      "epoch 6000 - loss: 0.1920\n",
      "epoch 7000 - loss: 0.1916\n",
      "epoch 8000 - loss: 0.1913\n",
      "epoch 9000 - loss: 0.1912\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    loss = 0\n",
    "    for j in range(len(X)):\n",
    "        forwards(X[j])\n",
    "        backwards(y[j])\n",
    "        loss += (y[j]-forwards(X[j]))**2\n",
    "    if i%1000 == 0: print('epoch {} - loss: {:04.4f}'.format(i, loss[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [1, 0, 0, 0]; y: 0.20; pred: 0.15\n",
      "X: [0, 1, 0, 0]; y: 0.30; pred: 0.39\n",
      "X: [0, 0, 1, 0]; y: 0.40; pred: 0.29\n",
      "X: [0, 0, 0, 1]; y: 0.50; pred: 0.34\n",
      "X: [0, 0, 0, 1]; y: 0.05; pred: 0.30\n",
      "X: [1, 0, 0, 0]; y: 0.10; pred: 0.16\n",
      "X: [0, 1, 0, 0]; y: 0.20; pred: 0.38\n",
      "X: [0, 0, 1, 0]; y: 0.30; pred: 0.30\n",
      "X: [0, 0, 0, 1]; y: 0.40; pred: 0.33\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    pred = forwards(X[i])\n",
    "    loss = (y[i]-pred)**2\n",
    "    print('X: {}; y: {:04.2f}; pred: {:04.2f}'.format(X[i], y[i], pred[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
