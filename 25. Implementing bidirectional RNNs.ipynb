{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's beneficial to run the data through the network in both directions. We call such networks bidirectional RNNs. In the following example, we will implement the same LSTM network as we've implemented previously, but this time we will use a bidirectional RNN to classify the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the IMDB dataset from Keras; load the data with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train seq: 25000\n",
      "Test seq: 25000\n"
     ]
    }
   ],
   "source": [
    "n_words = 1000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=n_words)\n",
    "print('Train seq: {}'.format(len(X_train)))\n",
    "print('Test seq: {}'.format(len(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print an example output of the training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train example: \n",
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "\n",
      "Test example: \n",
      "[1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 717]\n"
     ]
    }
   ],
   "source": [
    "print('Train example: \\n{}'.format(X_train[0]))\n",
    "print('\\nTest example: \\n{}'.format(X_test[0]))\n",
    "\n",
    "# Note: the data is already preprocessed (words are mapped to vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By padding the sequences, we prepare our input for our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pad sequences with max_len\n",
    "max_len = 200\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 50)           50000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 50)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               120800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 221,301\n",
      "Trainable params: 221,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define network architecture and compile\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_words, 50, input_length=max_len))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To prevent overfitting, we will be using early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', patience=3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the hyperparameters and start training our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 295s 15ms/step - loss: 0.6894 - acc: 0.5343 - val_loss: 0.6608 - val_acc: 0.6860\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 206s 10ms/step - loss: 0.5945 - acc: 0.6950 - val_loss: 0.4916 - val_acc: 0.7602\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 273s 14ms/step - loss: 0.4791 - acc: 0.7762 - val_loss: 0.4518 - val_acc: 0.7886\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 292s 15ms/step - loss: 0.4227 - acc: 0.8083 - val_loss: 0.4278 - val_acc: 0.8066\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 292s 15ms/step - loss: 0.4059 - acc: 0.8231 - val_loss: 0.3988 - val_acc: 0.8206\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 271s 14ms/step - loss: 0.3939 - acc: 0.8321 - val_loss: 0.4275 - val_acc: 0.8028\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 200s 10ms/step - loss: 0.3877 - acc: 0.8327 - val_loss: 0.4089 - val_acc: 0.8158\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 131s 7ms/step - loss: 0.3838 - acc: 0.8347 - val_loss: 0.3966 - val_acc: 0.8238\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 130s 6ms/step - loss: 0.3708 - acc: 0.8420 - val_loss: 0.3914 - val_acc: 0.8282\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 130s 7ms/step - loss: 0.3625 - acc: 0.8477 - val_loss: 0.3977 - val_acc: 0.8244\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 128s 6ms/step - loss: 0.3676 - acc: 0.8457 - val_loss: 0.4006 - val_acc: 0.8184\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 130s 7ms/step - loss: 0.3625 - acc: 0.8467 - val_loss: 0.3854 - val_acc: 0.8308\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 131s 7ms/step - loss: 0.3487 - acc: 0.8560 - val_loss: 0.3753 - val_acc: 0.8366\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 130s 6ms/step - loss: 0.3470 - acc: 0.8566 - val_loss: 0.3942 - val_acc: 0.8268\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 129s 6ms/step - loss: 0.3513 - acc: 0.8531 - val_loss: 0.3787 - val_acc: 0.8378\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 131s 7ms/step - loss: 0.3419 - acc: 0.8579 - val_loss: 0.3638 - val_acc: 0.8430\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 131s 7ms/step - loss: 0.3311 - acc: 0.8637 - val_loss: 0.3637 - val_acc: 0.8402\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 129s 6ms/step - loss: 0.3291 - acc: 0.8650 - val_loss: 0.3743 - val_acc: 0.8318\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 128s 6ms/step - loss: 0.3341 - acc: 0.8615 - val_loss: 0.3871 - val_acc: 0.8252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbdc2c74ac8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "n_epochs = 100\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the performance of our trained network on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 47s 2ms/step\n",
      "Accuracy on test set: 0.8307599997138977\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on test set: {}'.format(model.evaluate(X_test, y_test, batch_size=batch_size)[1]))\n",
    "\n",
    "# Accuracy on test set: 0.8391600004386902"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the network is able to retrieve some additional information from parsing the data in both directions. This results in a slightly higher test accuracy of 83.91%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
