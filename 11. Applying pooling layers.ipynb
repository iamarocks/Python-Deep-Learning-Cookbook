{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying pooling layers\n",
    "A popular optimization technique for CNNs is pooling layers. A pooling layer is a method to reduce the number of trainable parameters in a smart way. Two of the most commonly used pooling layers are average pooling and maximum (max) pooling.<br> In the first, for a specified block size the inputs are averaged and extracted. For the latter, the maximum value within a block is extracted. These pooling layers provide a translational invariance. In other words, the exact location of a feature is less relevant. Also, by reducing the number of trainable parameters we limit the complexity of the network, which should prevent overfitting. Another benefit is that it will reduce the training and inference time significantly. <br> In the next recipe, we will add max pooling layers to the CNN we've implemented in the previous recipe and at the same time we will increase the number of filters in the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the training data to represent grayscaled image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = X_train[0].shape[0], X_train[0].shape[1]\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')/255.\n",
    "X_test = X_test.astype('float32')/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(set(y_train))\n",
    "y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "y_test = np_utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CNN architecture and output the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 665,994\n",
      "Trainable params: 665,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the network hyperparameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the callback function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 132s 3ms/step - loss: 0.3387 - acc: 0.8901 - val_loss: 0.0572 - val_acc: 0.9823\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 225s 5ms/step - loss: 0.1015 - acc: 0.9697 - val_loss: 0.0407 - val_acc: 0.9869\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 233s 5ms/step - loss: 0.0724 - acc: 0.9786 - val_loss: 0.0491 - val_acc: 0.9858\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 236s 5ms/step - loss: 0.0610 - acc: 0.9816 - val_loss: 0.0366 - val_acc: 0.9902\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 241s 5ms/step - loss: 0.0533 - acc: 0.9842 - val_loss: 0.0303 - val_acc: 0.9916\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 235s 5ms/step - loss: 0.0445 - acc: 0.9869 - val_loss: 0.0291 - val_acc: 0.9914\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 227s 5ms/step - loss: 0.0420 - acc: 0.9878 - val_loss: 0.0264 - val_acc: 0.9929\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 227s 5ms/step - loss: 0.0396 - acc: 0.9881 - val_loss: 0.0276 - val_acc: 0.9921\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 229s 5ms/step - loss: 0.0346 - acc: 0.9894 - val_loss: 0.0276 - val_acc: 0.9927\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 234s 5ms/step - loss: 0.0336 - acc: 0.9903 - val_loss: 0.0238 - val_acc: 0.9932\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 231s 5ms/step - loss: 0.0305 - acc: 0.9908 - val_loss: 0.0308 - val_acc: 0.9916\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 244s 5ms/step - loss: 0.0276 - acc: 0.9916 - val_loss: 0.0251 - val_acc: 0.9936\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 232s 5ms/step - loss: 0.0296 - acc: 0.9909 - val_loss: 0.0265 - val_acc: 0.9927\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 300s 6ms/step - loss: 0.0250 - acc: 0.9921 - val_loss: 0.0281 - val_acc: 0.9935\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 343s 7ms/step - loss: 0.0247 - acc: 0.9922 - val_loss: 0.0284 - val_acc: 0.9931\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 365s 8ms/step - loss: 0.0223 - acc: 0.9930 - val_loss: 0.0318 - val_acc: 0.9927\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 330s 7ms/step - loss: 0.0218 - acc: 0.9931 - val_loss: 0.0306 - val_acc: 0.9930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5bb2485eb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the results on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0216482737543\n",
      "Test accuracy: 0.9943\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying a max pooling layer after each convolutional layer, we were able to decrease our error rate to 0.45 (a decrease of 36% compared to our previous error rate of 0.70). Moreover, by applying max pooling the number of trainable parameters is reduced significantly to 665,994. As a result, 1 epoch takes just around 11 seconds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
